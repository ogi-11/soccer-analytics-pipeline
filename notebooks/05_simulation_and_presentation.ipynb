{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/final_rolling.csv')\n",
    "df2 = pd.read_csv('/match_by_match.csv')\n",
    "df3= pd.read_csv('/prem_with_elo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prem_df = (df2['Comp'] == 'Premier League')\n",
    "prem_df = df2[prem_df]\n",
    "prem_df = prem_df.drop(['Match Report', 'Notes', 'Time', 'Day'], axis=1)\n",
    "prem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee38bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_map = prem_df['Date']\n",
    "\n",
    "df['Date'] = df['original_index'].map(date_map)\n",
    "\n",
    "new_order = [\n",
    "    'Season',\n",
    "    'Team',\n",
    "    'Date',\n",
    "    'original_index',\n",
    "    'xG',\n",
    "    'xG Difference',\n",
    "    'xG_roll_avg',\n",
    "    'xG_diff_roll_avg',\n",
    "    'Points',\n",
    "    'Elo'\n",
    "]\n",
    "\n",
    "final_dataset = df[new_order]\n",
    "\n",
    "final_dataset.to_csv('/Users/ogizelenovic/Downloads/final_rolling.csv', encoding='utf-8-sig', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8452426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    player_stats_df = pd.read_csv('/MASTER.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Could not find a file.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- initial data cleaning and preparation\n",
    "print(\"\\nStep 1: Preparing source data...\")\n",
    "df3['Date'] = pd.to_datetime(df3['Date'])\n",
    "final_dataset['Date'] = pd.to_datetime(final_dataset['Date'])\n",
    "player_stats_df['Season'] = player_stats_df['Season'].astype(str)\n",
    "\n",
    "# --- aggregateing player stats to create team-level stats\n",
    "print(\"Step 2: Aggregating player stats...\")\n",
    "\n",
    "numeric_cols = ['Age', 'Min', 'Gls', 'Ast', 'npxG', 'xAG', 'PrgC', 'PrgP']\n",
    "for col in numeric_cols:\n",
    "    player_stats_df[col] = pd.to_numeric(player_stats_df[col], errors='coerce')\n",
    "\n",
    "significant_players = player_stats_df[player_stats_df['Min'] > 450].copy()\n",
    "\n",
    "team_level_features = significant_players.groupby(['Season', 'Squad']).agg(\n",
    "    squad_avg_age=('Age', 'mean'),\n",
    "    squad_total_npxg=('npxG', 'sum'),\n",
    "    squad_total_prgp=('PrgP', 'sum'),\n",
    "    squad_total_gls=('Gls', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# --- create final dataframe\n",
    "model_df = df3.copy()\n",
    "rolling_features_to_merge = final_dataset[['Season', 'Team', 'Date', 'xG_roll_avg', 'xG_diff_roll_avg']].copy()\n",
    "\n",
    "model_df = pd.merge(model_df, rolling_features_to_merge, on=['Season', 'Date', 'Team'], how='left')\n",
    "model_df.rename(columns={'xG_roll_avg': 'xG_roll_avg_home', 'xG_diff_roll_avg': 'xG_diff_roll_avg_home'}, inplace=True)\n",
    "\n",
    "model_df = pd.merge(model_df, rolling_features_to_merge, left_on=['Season', 'Date', 'Opponent'], right_on=['Season', 'Date', 'Team'], how='left', suffixes=('_home_team', '_away_team'))\n",
    "model_df.rename(columns={'xG_roll_avg': 'xG_roll_avg_away', 'xG_diff_roll_avg': 'xG_diff_roll_avg_away'}, inplace=True)\n",
    "model_df.drop(columns=['Team_away_team'], inplace=True)\n",
    "model_df.rename(columns={'Team_home_team': 'Team'}, inplace=True)\n",
    "\n",
    "model_df = pd.merge(model_df, team_level_features, left_on=['Season', 'Team'], right_on=['Season', 'Squad'], how='left')\n",
    "model_df.drop(columns=['Squad'], inplace=True)\n",
    "\n",
    "model_df = pd.merge(model_df, team_level_features, left_on=['Season', 'Opponent'], right_on=['Season', 'Squad'], how='left', suffixes=('_home', '_away'))\n",
    "model_df.drop(columns=['Squad'], inplace=True)\n",
    "\n",
    "# --- feature engineering\n",
    "print(\"Step 4: Engineering final features...\")\n",
    "\n",
    "model_df['elo_diff'] = model_df['Home Elo'] - model_df['Away Elo']\n",
    "model_df['xg_roll_avg_diff'] = model_df['xG_roll_avg_home'] - model_df['xG_roll_avg_away']\n",
    "model_df['squad_total_npxg_diff'] = model_df['squad_total_npxg_home'] - model_df['squad_total_npxg_away']\n",
    "\n",
    "features = [\n",
    "    'Home Elo', 'Away Elo', 'elo_diff',\n",
    "    'xG_roll_avg_home', 'xG_roll_avg_away', 'xg_roll_avg_diff',\n",
    "    'xG_diff_roll_avg_home', 'xG_diff_roll_avg_away',\n",
    "    'squad_avg_age_home', 'squad_total_npxg_home', 'squad_total_prgp_home',\n",
    "    'squad_avg_age_away', 'squad_total_npxg_away', 'squad_total_prgp_away',\n",
    "    'squad_total_npxg_diff'\n",
    "]\n",
    "X = model_df[features]\n",
    "y = model_df['Result']\n",
    "\n",
    "X.fillna(0, inplace=True)\n",
    "y = y[X.index]\n",
    "\n",
    "# --- train binary classification model\n",
    "print(\"Step 5 (Revised): Training and evaluating a Binary (Win/No-Win) Model...\")\n",
    "\n",
    "# ### CHANGE 1: Import the new model and tools\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_binary = y.map({'W': 1, 'D': 0, 'L': 0})\n",
    "\n",
    "\n",
    "# --- cross validation split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "accuracies = []\n",
    "roc_aucs = []\n",
    "all_true = []\n",
    "all_preds = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_binary.iloc[train_index], y_binary.iloc[test_index]\n",
    "\n",
    "    imputation_values = X_train.mean()\n",
    "    X_train = X_train.fillna(imputation_values)\n",
    "    X_test = X_test.fillna(imputation_values)\n",
    "\n",
    "    # max_iter is increased to ensure convergence.\n",
    "    binary_model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    \n",
    "    # fitting model on training data\n",
    "    binary_model.fit(X_train, y_train)\n",
    "\n",
    "    # predicting on unseen data\n",
    "    predicted_outcomes = binary_model.predict(X_test)\n",
    "    predicted_probabilities = binary_model.predict_proba(X_test)[:, 1] # Get prob of class '1' (Win)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predicted_outcomes)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # ROC AUC measure\n",
    "    roc_auc = roc_auc_score(y_test, predicted_probabilities)\n",
    "    roc_aucs.append(roc_auc)\n",
    "    \n",
    "    all_true.extend(y_test)\n",
    "    all_preds.extend(predicted_outcomes)\n",
    "    \n",
    "    print(f\"Fold Accuracy: {accuracy:.4f} | Fold ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# --- final eval\n",
    "print(\"\\n--- Binary Model Out-of-Sample Performance ---\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(accuracies):.4f} ({np.mean(accuracies):.2%})\")\n",
    "print(f\"Average ROC AUC across all folds:  {np.mean(roc_aucs):.4f}\")\n",
    "\n",
    "print(classification_report(all_true, all_preds, target_names=['No-Win', 'Win']))\n",
    "\n",
    "# --- feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'coefficient': binary_model.coef_[0]\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importance (from last fold's model) ---\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm # A progress bar for the simulation\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "try:\n",
    "    player_stats_df = pd.read_csv('/MASTER.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Could not find a file.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- final Elo state\n",
    "print(\"\\nStep 1: Calculating final Elo ratings from historical data...\")\n",
    "\n",
    "# --- data prep\n",
    "df3['Date'] = pd.to_datetime(df3['Date'])\n",
    "K = 25\n",
    "HFA = 40\n",
    "result_points_elo = {'W': 1, 'D': 0.5, 'L': 0}\n",
    "all_historical_teams = sorted(list(set(df3['Team'].unique()) | set(df3['Opponent'].unique())))\n",
    "elo_score = {team: 1500 for team in all_historical_teams}\n",
    "previous_season = None\n",
    "\n",
    "# --- looping through matches to get final elos\n",
    "historical_matches = df3[df3['Date'] < '2024-08-01'].copy().sort_values('Date')\n",
    "\n",
    "for _, row in historical_matches.iterrows():\n",
    "    current_season = row['Season']\n",
    "    if current_season != previous_season and previous_season is not None:\n",
    "        for team in elo_score: elo_score[team] = 1500 + (2/3) * (elo_score[team] - 1500)\n",
    "    previous_season = current_season\n",
    "    \n",
    "    hometeam, awayteam = row['Team'], row['Opponent']\n",
    "    elo_h, elo_a = elo_score[hometeam], elo_score[awayteam]\n",
    "    \n",
    "    expected_h = 1 / (1 + 10**((elo_a - (elo_h + HFA)) / 400))\n",
    "    actual_h = result_points_elo[row['Result']]\n",
    "    \n",
    "    elo_score[hometeam] += K * (actual_h - expected_h)\n",
    "    elo_score[awayteam] -= K * (actual_h - expected_h)\n",
    "\n",
    "# --- assemble features and train OrderedModel\n",
    "\n",
    "# --- assemble the full feature set (model_df) ---\n",
    "final_dataset['Date'] = pd.to_datetime(final_dataset['Date'])\n",
    "player_stats_df['Season'] = player_stats_df['Season'].astype(str)\n",
    "numeric_cols = ['Age', 'Min', 'Gls', 'Ast', 'npxG', 'xAG', 'PrgC', 'PrgP']\n",
    "for col in numeric_cols: \n",
    "    player_stats_df[col] = pd.to_numeric(player_stats_df[col], errors='coerce')\n",
    "significant_players = player_stats_df[player_stats_df['Min'] > 450]\n",
    "team_level_features = significant_players.groupby(['Season', 'Squad']).agg(\n",
    "    squad_avg_age=('Age', 'mean'), \n",
    "    squad_total_npxg=('npxG', 'sum'),\n",
    "    squad_total_prgp=('PrgP', 'sum'), \n",
    "    squad_total_gls=('Gls', 'sum')).reset_index()\n",
    "\n",
    "model_df = df3.copy()\n",
    "rolling_features_to_merge = final_dataset[['Season', 'Team', 'Date', 'xG_roll_avg', 'xG_diff_roll_avg']]\n",
    "model_df = pd.merge(model_df, rolling_features_to_merge, on=['Season', 'Date', 'Team'], how='left')\n",
    "model_df.rename(columns={'xG_roll_avg': 'xG_roll_avg_home', 'xG_diff_roll_avg': 'xG_diff_roll_avg_home'}, inplace=True)\n",
    "model_df = pd.merge(model_df, rolling_features_to_merge, left_on=['Season', 'Date', 'Opponent'], right_on=['Season', 'Date', 'Team'], how='left', suffixes=('_home_team', '_away_team'))\n",
    "model_df.rename(columns={'xG_roll_avg': 'xG_roll_avg_away', 'xG_diff_roll_avg': 'xG_diff_roll_avg_away'}, inplace=True)\n",
    "model_df.drop(columns=['Team_away_team'], inplace=True)\n",
    "model_df.rename(columns={'Team_home_team'   : 'Team'}, inplace=True)\n",
    "model_df = pd.merge(model_df, team_level_features, left_on=['Season', 'Team'], right_on=['Season', 'Squad'], how='left')\n",
    "model_df.drop(columns=['Squad'], inplace=True)\n",
    "model_df = pd.merge(model_df, team_level_features, left_on=['Season', 'Opponent'], right_on=['Season', 'Squad'], how='left', suffixes=('_home', '_away'))\n",
    "model_df.drop(columns=['Squad'], inplace=True)\n",
    "\n",
    "# --- engineer features and define X, y \n",
    "model_df['elo_diff'] = model_df['Home Elo'] - model_df['Away Elo']\n",
    "model_df['xg_roll_avg_diff'] = model_df['xG_roll_avg_home'] - model_df['xG_roll_avg_away']\n",
    "model_df['squad_total_npxg_diff'] = model_df['squad_total_npxg_home'] - model_df['squad_total_npxg_away']\n",
    "\n",
    "features = [\n",
    "    'Home Elo', 'Away Elo', 'elo_diff', 'xG_roll_avg_home', 'xG_roll_avg_away', 'xg_roll_avg_diff',\n",
    "    'xG_diff_roll_avg_home', 'xG_diff_roll_avg_away', 'squad_avg_age_home', 'squad_total_npxg_home',\n",
    "    'squad_total_prgp_home', 'squad_avg_age_away', 'squad_total_npxg_away', 'squad_total_prgp_away',\n",
    "    'squad_total_npxg_diff']\n",
    "X = model_df[features]\n",
    "y = model_df['Result']\n",
    "X.fillna(0, inplace=True)\n",
    "y = y[X.index]\n",
    "\n",
    "# --- train ifnal OrderedModel\n",
    "cat_type = CategoricalDtype(categories=['L', 'D', 'W'], ordered=True)\n",
    "y_ordered = y.astype(cat_type).cat.codes\n",
    "trained_model_result = OrderedModel(y_ordered, X).fit(method='bfgs', disp=0)\n",
    "print(\"Predictive model trained successfully on all historical data.\")\n",
    "\n",
    "\n",
    "# --- setting up and running 2024-25 season\n",
    "\n",
    "# --- initial state for sim\n",
    "final_elo_ratings = elo_score.copy()\n",
    "last_known_form = final_dataset[final_dataset['Season']=='2023-2024'].sort_values('Date').drop_duplicates('Team', keep='last').set_index('Team')[['xG_roll_avg', 'xG_diff_roll_avg']].to_dict('index')\n",
    "squad_features_23_24 = team_level_features[team_level_features['Season'] == '2023-2024'].set_index('Squad').to_dict('index')\n",
    "\n",
    "# --- standardising names across the board\n",
    "teams_24_25_raw = ['AFC Bournemouth', 'Arsenal', 'Aston Villa', 'Brentford', 'Brighton & Hove Albion', 'Chelsea', 'Crystal Palace', 'Everton', 'Fulham', 'Ipswich Town', 'Leicester City', 'Liverpool', 'Manchester City', 'Manchester United', 'Newcastle United', 'Nottingham Forest', 'Southampton', 'Tottenham Hotspur', 'West Ham United', 'Wolverhampton Wanderers']\n",
    "name_map = {'AFC Bournemouth': 'Bournemouth', 'Brighton & Hove Albion': 'Brighton-and-Hove-Albion', 'Aston Villa': 'Aston-Villa', 'Ipswich Town': 'Ipswich-Town', 'Leicester City': 'Leicester-City', 'Manchester City': 'Manchester-City', 'Manchester United': 'Manchester-United', 'Newcastle United': 'Newcastle-United', 'Nottingham Forest': 'Nottingham-Forest', 'Tottenham Hotspur': 'Tottenham-Hotspur', 'West Ham United': 'West-Ham-United', 'Wolverhampton Wanderers': 'Wolverhampton-Wanderers', 'Crystal Palace': 'Crystal-Palace'}\n",
    "teams_24_25 = [name_map.get(team, team) for team in teams_24_25_raw]\n",
    "\n",
    "# --- creating features and running sim loop\n",
    "fixtures = pd.DataFrame([{'HomeTeam': h, 'AwayTeam': a} for h in teams_24_25 for a in teams_24_25 if h != a]).sample(frac=1).reset_index(drop=True)\n",
    "N_SIMULATIONS = 100\n",
    "season_winner_counts = {team: 0 for team in teams_24_25}\n",
    "\n",
    "for i in tqdm(range(N_SIMULATIONS)):\n",
    "    sim_elo = final_elo_ratings.copy()\n",
    "    sim_table = {team: {'points': 0, 'gd': 0} for team in teams_24_25}\n",
    "    sim_form = {team: {'xG_history': deque(maxlen=5), 'xGA_history': deque(maxlen=5)} for team in teams_24_25}\n",
    "\n",
    "    for team in teams_24_25:\n",
    "        if team not in sim_elo: sim_elo[team] = 1450\n",
    "        if team not in squad_features_23_24: squad_features_23_24[team] = {'squad_avg_age': 26.5, 'squad_total_npxg': 38.0, 'squad_total_prgp': 1600.0, 'squad_total_gls': 45.0}\n",
    "\n",
    "    for team in sim_elo: sim_elo[team] = 1500 + (2/3) * (sim_elo[team] - 1500)\n",
    "\n",
    "    for _, match in fixtures.iterrows():\n",
    "        home_team, away_team = match['HomeTeam'], match['AwayTeam']\n",
    "        elo_h, elo_a = sim_elo[home_team], sim_elo[away_team]\n",
    "        \n",
    "        home_xG_roll_avg = np.mean(sim_form[home_team]['xG_history']) if sim_form[home_team]['xG_history'] else 1.3\n",
    "        home_xGA_roll_avg = np.mean(sim_form[home_team]['xGA_history']) if sim_form[home_team]['xGA_history'] else 1.3\n",
    "        away_xG_roll_avg = np.mean(sim_form[away_team]['xG_history']) if sim_form[away_team]['xG_history'] else 1.3\n",
    "        away_xGA_roll_avg = np.mean(sim_form[away_team]['xGA_history']) if sim_form[away_team]['xGA_history'] else 1.3\n",
    "        \n",
    "        match_features = pd.DataFrame([{\n",
    "            'Home Elo': elo_h, 'Away Elo': elo_a, 'elo_diff': elo_h - elo_a,\n",
    "            'xG_roll_avg_home': home_xG_roll_avg, 'xG_diff_roll_avg_home': home_xG_roll_avg - home_xGA_roll_avg,\n",
    "            'xG_roll_avg_away': away_xG_roll_avg, 'xG_diff_roll_avg_away': away_xG_roll_avg - away_xGA_roll_avg,\n",
    "            'xg_roll_avg_diff': home_xG_roll_avg - away_xG_roll_avg,\n",
    "            'squad_avg_age_home': squad_features_23_24[home_team]['squad_avg_age'], 'squad_total_npxg_home': squad_features_23_24[home_team]['squad_total_npxg'],\n",
    "            'squad_total_prgp_home': squad_features_23_24[home_team]['squad_total_prgp'], 'squad_avg_age_away': squad_features_23_24[away_team]['squad_avg_age'],\n",
    "            'squad_total_npxg_away': squad_features_23_24[away_team]['squad_total_npxg'], 'squad_total_prgp_away': squad_features_23_24[away_team]['squad_total_prgp'],\n",
    "            'squad_total_npxg_diff': squad_features_23_24[home_team]['squad_total_npxg'] - squad_features_23_24[away_team]['squad_total_npxg']\n",
    "        }])[features].fillna(0)\n",
    "\n",
    "        probabilities = trained_model_result.predict(match_features).iloc[0].values\n",
    "        simulated_result = np.random.choice([0, 1, 2], p=probabilities) # 0=L, 1=D, 2=W\n",
    "        \n",
    "        if simulated_result == 2: points_h, points_a, gd_h = 3, 0, 1\n",
    "        elif simulated_result == 1: points_h, points_a, gd_h = 1, 1, 0\n",
    "        else: points_h, points_a, gd_h = 0, 3, -1\n",
    "            \n",
    "        sim_table[home_team]['points'] += points_h\n",
    "        sim_table[away_team]['points'] += points_a\n",
    "        sim_table[home_team]['gd'] += gd_h\n",
    "        sim_table[away_team]['gd'] -= gd_h\n",
    "        \n",
    "        actual_h = {2: 1, 1: 0.5, 0: 0}[simulated_result]\n",
    "        expected_h = 1 / (1 + 10**((elo_a - (elo_h + HFA)) / 400))\n",
    "        sim_elo[home_team] += K * (actual_h - expected_h)\n",
    "        sim_elo[away_team] -= K * (actual_h - expected_h)\n",
    "        \n",
    "        mock_xg_home = max(0.2, 1.25 + (elo_h - 1500)/200 - (elo_a - 1500)/250)\n",
    "        mock_xg_away = max(0.2, 1.25 + (elo_a - 1500)/200 - (elo_h - 1500)/250)\n",
    "        sim_form[home_team]['xG_history'].append(mock_xg_home)\n",
    "        sim_form[home_team]['xGA_history'].append(mock_xg_away)\n",
    "        sim_form[away_team]['xG_history'].append(mock_xg_away)\n",
    "        sim_form[away_team]['xGA_history'].append(mock_xg_home)\n",
    "        \n",
    "    final_table = sorted(sim_table.items(), key=lambda item: (item[1]['points'], item[1]['gd']), reverse=True)\n",
    "    winner = final_table[0][0]\n",
    "    season_winner_counts[winner] += 1\n",
    "\n",
    "# --- presenting results\n",
    "print(\"\\n--- Predicted 2024-25 Season Winner Probabilities ---\")\n",
    "winner_probabilities = {team: (count / N_SIMULATIONS) * 100 for team, count in season_winner_counts.items()}\n",
    "sorted_winners = sorted(winner_probabilities.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "reverse_name_map = {v: k for k, v in name_map.items()}\n",
    "for team_std, prob in sorted_winners:\n",
    "    if prob > 0:\n",
    "        team_display = reverse_name_map.get(team_std, team_std)\n",
    "        print(f\"{team_display:<30} {prob:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
